* idea

We then select the top N fields after duplicate reduction. Those are the fields that occure the most in all the samples.
This will allow us to unite the different datasets together with common elements.
These united datasets can then be used to train autogluon. 

The idea is that we can convert the asts to json, flatten them into parquet.
this gives us many datasets with different features. Each dataset has one row,
and that row contains a column for each field, where the field name is the path
to reach that field in the tree, that is the flattend json.
We then remove the numbers from the fieldnames to produce shorter fieldnames
removing the positional information from the fieldnames, this merges many fields together.
Those poisitons we will want later as a value of sorts.

Next we can consider the columns and realize that some of the values also reference the columes.
There is a relationship between columns, and a relationship between columnes and values.

If we vectorize them both, we will find an overlap. We can vectorize the columns and the values and then match them based on that.

This will give us an indication of the areas, or a layout of the data into the vector space.
We will then want to use this information to embed the datasets row by row.
We can cluster the rows together and embed them as well based on cluster.

This should give us some clustering and partitioning.
There will be relationships between different modules and functions as well.

** what we are looking for :

1. descriptions of the fields in the values that match the columns.
   Some of the values will match the columns exactly. We want to basically vectorize them both and join the most similar to each other.

2. longer paths in the fields will be compounds. We can shorten or longen the paths of the fields.

* data collected
python ./reflect.py > ast_json.json
jq . ast_json.json >ast_json_pretty.json
grep "\"_type\"" ast_json_pretty.json  | cut -d: -f2 | sort |uniq -c | sort -n |tail

|  2817 | "Expr",      |
|  4102 | "arg",       |
|  5543 | "Assign",    |
|  8700 | "Store"      |
|  9233 | "Call",      |
| 10076 | "Attribute", |
| 10266 | "Tuple",     |
| 40225 | "Name",      |
| 55209 | "Load"       |
| 66254 | "Constant",  |

 * Some of the relationships, this shows the count, the
   name of the node and the parent context.
    
|  8177 | Dump | Return | function_body |
|  8629 | Dump | If     | function_body |
| 10839 | Dump | Expr   | function_body |
| 15382 | Dump | Assign | function_body |

* next steps

1. look at which fields occur together, many of them should be null, rank them.
   compare those groups, define variations.
   find rules to collapse smaller groups into larger.

** First report

Here are the first 1000 rows of the top types in all the samples
Note that not all the files got converted to datasets but this is good enough

The first thing that you will see are the body_N_types that are top level items.
I think we could also collapse the number and replace them

created rank_fields_strings.py

* Reports

* report with numbers
|----------------------------------------------+-------+
| Type                                         | Count |
|----------------------------------------------+-------+
| _type                                        |  1640 |
| type_ignores                                 |  1640 |
| body_0__type                                 |  1533 |
| body_1__type                                 |  1512 |
| body_2__type                                 |  1474 |
| body_1_names_0__type                         |  1453 |
| body_3__type                                 |  1421 |
| body_2_names_0__type                         |  1363 |
| body_4__type                                 |  1334 |
| body_5__type                                 |  1233 |
| body_3_names_0__type                         |  1202 |
| body_0_names_0__type                         |  1134 |

** Report without numbers
this report gives a more compact representation
|-------+--------------------------------------------------------------------------------+--------|
|  Type | Count                                                                          |        |
|    10 | body_N_body_N_body_N__type                                                     | 116809 |
|     8 | body_N_body_N__type                                                            |  89679 |
|    14 | body_N_body_N_body_N_value__type                                               |  87969 |
|    15 | body_N_body_N_body_N_value_args_N__type                                        |  75187 |
|    13 | body_N_body_N_body_N_type_comment                                              |  67042 |
|    17 | body_N_body_N_body_N_value_func__type                                          |  62897 |
|    18 | body_N_body_N_body_N_value_func_ctx__type                                      |  62426 |
|    46 | body_N_body_N_type_comment                                                     |  60657 |
|    12 | body_N_body_N_body_N_targets_N_ctx__type                                       |  59814 |
|    11 | body_N_body_N_body_N_targets_N__type                                           |  59814 |
|    16 | body_N_body_N_body_N_value_args_N_ctx__type                                    |  52048 |
|    30 | body_N_body_N_value__type                                                      |  51231 |
|    31 | body_N_body_N_value_args_N__type                                               |  41645 |
|    19 | body_N_body_N_body_N_value_func_value__type                                    |  39620 |
|   284 | body_N_body_N_args_args_N_type_comment                                         |  38184 |
|   283 | body_N_body_N_args_args_N__type                                                |  38184 |
|    45 | body_N_body_N_targets_N_ctx__type                                              |  37081 |


** More Ideas

After running this for some time.
1. we want to be able to deploy this work to different cloud providers.
2. We want to be able to create features directly from python and feed it to
   autogluon. Take the autogluon features and convert them back into python code.
   a two way street of morphisms. Basically we want to shorten the path.
3. We want to classify code as something that inputs simple protocols/ programs
   or complex programs. A compiler takes a language that is complex. A regex is simpler.
   If we can analyse a program and determine how complex it is, measure its fitness or
   quality.

4. We can modify the programs to emit the internal representations.
